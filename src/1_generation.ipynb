{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f767425b",
   "metadata": {},
   "source": [
    "# Gerenate using ChatGPT\n",
    "We will generate 10 texts for each abstract across two temperatures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b218d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a7a0799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  \n",
    "\n",
    "# dan jurafsky's string2string library\n",
    "from string2string.similarity import JaroSimilarity\n",
    "\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "from prompting.prompts import DataTemplates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8e41529b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../abstracts_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "eafbdb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = ChatPromptTemplate.from_messages(DataTemplates().medicine_abstract_prompt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b236c571",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b906323e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_row(text: str, temperature) -> str:\n",
    "    llm = ChatOpenAI(temperature=temperature)\n",
    "    chat = LLMChain(llm=llm, prompt=template)\n",
    "    output = chat.run({\"text\": text})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "12d68b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = [0.7, 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c3ca4add",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d675412aa67423dc58f2d9ee3638cad5 in your message.).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID fe226e1ef25567f2e19d9578176f234f in your message.).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID acbec8f754363f134d67b88b6bf99442 in your message.).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 97f39f74c77ffab2cc4a8cc13a714f43 in your message.).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 0e5615327fc3c8596b932494221504d2 in your message.).\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "def generate_wrapper(row, t):\n",
    "    text = row[\"texts\"]\n",
    "    return {\"temperature\": t, \"text\": generate_row(text, t)}\n",
    "\n",
    "generations = []\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    for j in range(10):\n",
    "        for t in temperatures:\n",
    "            # Prepare arguments for each row\n",
    "            args = [(row, t) for index, row in df.iterrows()]b\n",
    "\n",
    "            # Execute the generate_wrapper function with each row in a separate thread\n",
    "            results = list(executor.map(lambda params: generate_wrapper(*params), args))\n",
    "            \n",
    "            # Add results to generations list\n",
    "            generations.extend(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acaf6c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d6b34654",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generations = []\n",
    "# while j < 10:\n",
    "#     for t in temperatures:\n",
    "#         for i, row in df.iterrows():\n",
    "#             text = row[\"texts\"]\n",
    "#             generations.append({\"temperature\": t, \"text\": generate_row(text, t)})\n",
    "#     j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a33f8b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(generations).reset_index()\n",
    "df2[\"index\"] = df2[\"index\"].apply(lambda x: x % 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "dd26b17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_labels = pd.read_csv(\"../data/qids.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "423311a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_labels.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "680b940c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.merge(id_labels[[\"index\", \"HITId\"]], on = \"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "05c63c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[temperatures] = df2.pivot(index=\"index\", columns=\"temperature\", values=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7208c9c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e1a73392",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv(\"../data/generated_outputs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe57df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34d0b5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = pd.read_csv(\"../data/generated_outputs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4322ff63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d8dc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [\"ai\", \"ai\", \"human\", \"human\", \"human\", \"AI\", \"AI\", \"AI\", \"AI\", \"human\"]\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
